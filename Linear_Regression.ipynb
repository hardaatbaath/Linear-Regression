{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PzLlGDC0xCrm"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "regression = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A93VJaqzwPv",
        "outputId": "737b025b-c86a-4f2c-8e61-1b8508c2a1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X : (9568, 4)\n",
            "Shape of Y : (9568,)\n"
          ]
        }
      ],
      "source": [
        "# Importing and collecting data\n",
        "linreg=pd.read_csv(\"./LinReg_Data.csv\")\n",
        "x=linreg.drop(columns=['PE'])\n",
        "y=linreg['PE']\n",
        "x=np.array(x)\n",
        "y=np.array(y)\n",
        "print(f\"Shape of X : {x.shape}\")\n",
        "print(f\"Shape of Y : {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uS8z_x58YObv"
      },
      "outputs": [],
      "source": [
        "# Normalising Data\n",
        "x = (x - np.mean(x))/np.std(x)\n",
        "y = (y - np.mean(y))/np.std(y)\n",
        "\n",
        "#Diving the data into training and testing set\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GCazagI6ipf",
        "outputId": "b3d6229b-374e-49e7-c620-ee44bd57e12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of W : (1, 4)\n",
            "Shape of b : (1, 1)\n"
          ]
        }
      ],
      "source": [
        "n_total = x.shape[0] # Total No. of samples\n",
        "n_samples = x_train.shape[0] # No. of samples taken for training\n",
        "n_features = x.shape[1]  # No. of features / No. of values in X\n",
        "n_targets = 1   # No. of target variables / No. of values in y\n",
        "\n",
        "# Creating the variables\n",
        "W = np.zeros((n_targets, n_features))\n",
        "b = np.zeros((n_targets,1))\n",
        "\n",
        "print(f\"Shape of W : {W.shape}\")\n",
        "print(f\"Shape of b : {b.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2zZXWLYw7zRf"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function\n",
        "def loss_fn(y_hat, y):\n",
        "    return (1/n_samples) * np.sum((y_hat - y)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GHJYze_CCKMw",
        "outputId": "23628f8d-c3a3-45fa-9fe8-a6d98abcd328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.8448056581351859\n",
            "Epoch: 2, Loss: 0.5346031619554391\n",
            "Epoch: 3, Loss: 0.3944064196167968\n",
            "Epoch: 4, Loss: 0.3238903286628381\n",
            "Epoch: 5, Loss: 0.2840162865223729\n",
            "Epoch: 6, Loss: 0.2589399158107953\n",
            "Epoch: 7, Loss: 0.24179242406224327\n",
            "Epoch: 8, Loss: 0.22931808229508957\n",
            "Epoch: 9, Loss: 0.2198085021437337\n",
            "Epoch: 10, Loss: 0.21227736456615023\n",
            "Epoch: 11, Loss: 0.2061109955637217\n",
            "Epoch: 12, Loss: 0.2009077163121665\n",
            "Epoch: 13, Loss: 0.19639626544375405\n",
            "Epoch: 14, Loss: 0.19239005524803043\n",
            "Epoch: 15, Loss: 0.18875933329016834\n",
            "Epoch: 16, Loss: 0.18541323680904898\n",
            "Epoch: 17, Loss: 0.18228779984989163\n",
            "Epoch: 18, Loss: 0.17933776905189824\n",
            "Epoch: 19, Loss: 0.17653095323042609\n",
            "Epoch: 20, Loss: 0.1738442976221368\n",
            "Epoch: 21, Loss: 0.17126114713708332\n",
            "Epoch: 22, Loss: 0.16876933503852973\n",
            "Epoch: 23, Loss: 0.16635984678132495\n",
            "Epoch: 24, Loss: 0.1640258854120745\n",
            "Epoch: 25, Loss: 0.16176221762127627\n",
            "Epoch: 26, Loss: 0.15956471604926573\n",
            "Epoch: 27, Loss: 0.15743003886547302\n",
            "Epoch: 28, Loss: 0.15535540537818585\n",
            "Epoch: 29, Loss: 0.15333843882625092\n",
            "Epoch: 30, Loss: 0.15137705617065894\n",
            "Epoch: 31, Loss: 0.14946939076611046\n",
            "Epoch: 32, Loss: 0.14761373803390232\n",
            "Epoch: 33, Loss: 0.14580851722498242\n",
            "Epoch: 34, Loss: 0.14405224443846235\n",
            "Epoch: 35, Loss: 0.14234351351371627\n",
            "Epoch: 36, Loss: 0.14068098243077584\n",
            "Epoch: 37, Loss: 0.1390633635649297\n",
            "Epoch: 38, Loss: 0.13748941663896605\n",
            "Epoch: 39, Loss: 0.13595794356457724\n",
            "Epoch: 40, Loss: 0.1344677846078354\n",
            "Epoch: 41, Loss: 0.13301781548389222\n",
            "Epoch: 42, Loss: 0.1316069451050618\n",
            "Epoch: 43, Loss: 0.13023411378964478\n",
            "Epoch: 44, Loss: 0.12889829179698248\n",
            "Epoch: 45, Loss: 0.12759847809485197\n",
            "Epoch: 46, Loss: 0.1263336992936618\n",
            "Epoch: 47, Loss: 0.1251030087017157\n",
            "Epoch: 48, Loss: 0.12390548546961372\n",
            "Epoch: 49, Loss: 0.12274023380150527\n",
            "Epoch: 50, Loss: 0.12160638221758967\n",
            "Epoch: 51, Loss: 0.12050308285698065\n",
            "Epoch: 52, Loss: 0.11942951081323787\n",
            "Epoch: 53, Loss: 0.11838486349719005\n",
            "Epoch: 54, Loss: 0.11736836002319646\n",
            "Epoch: 55, Loss: 0.11637924061611205\n",
            "Epoch: 56, Loss: 0.11541676603694283\n",
            "Epoch: 57, Loss: 0.11448021702574096\n",
            "Epoch: 58, Loss: 0.11356889376061753\n",
            "Epoch: 59, Loss: 0.11268211533199979\n",
            "Epoch: 60, Loss: 0.11181921923146242\n",
            "Epoch: 61, Loss: 0.11097956085454484\n",
            "Epoch: 62, Loss: 0.11016251301707113\n",
            "Epoch: 63, Loss: 0.10936746548454836\n",
            "Epoch: 64, Loss: 0.10859382451423512\n",
            "Epoch: 65, Loss: 0.1078410124095492\n",
            "Epoch: 66, Loss: 0.10710846708643809\n",
            "Epoch: 67, Loss: 0.10639564165141978\n",
            "Epoch: 68, Loss: 0.1057020039909743\n",
            "Epoch: 69, Loss: 0.10502703637197607\n",
            "Epoch: 70, Loss: 0.10437023505288584\n",
            "Epoch: 71, Loss: 0.10373110990542547\n",
            "Epoch: 72, Loss: 0.10310918404643682\n",
            "Epoch: 73, Loss: 0.10250399347968028\n",
            "Epoch: 74, Loss: 0.10191508674729781\n",
            "Epoch: 75, Loss: 0.101342024590683\n",
            "Epoch: 76, Loss: 0.1007843796205132\n",
            "Epoch: 77, Loss: 0.10024173599569898\n",
            "Epoch: 78, Loss: 0.09971368911100882\n",
            "Epoch: 79, Loss: 0.09919984529313865\n",
            "Epoch: 80, Loss: 0.09869982150500328\n",
            "Epoch: 81, Loss: 0.09821324505802634\n",
            "Epoch: 82, Loss: 0.09773975333221054\n",
            "Epoch: 83, Loss: 0.09727899350378565\n",
            "Epoch: 84, Loss: 0.09683062228022454\n",
            "Epoch: 85, Loss: 0.09639430564242922\n",
            "Epoch: 86, Loss: 0.09596971859390022\n",
            "Epoch: 87, Loss: 0.09555654491668616\n",
            "Epoch: 88, Loss: 0.09515447693394856\n",
            "Epoch: 89, Loss: 0.09476321527894929\n",
            "Epoch: 90, Loss: 0.09438246867028922\n",
            "Epoch: 91, Loss: 0.09401195369323848\n",
            "Epoch: 92, Loss: 0.09365139458697876\n",
            "Epoch: 93, Loss: 0.09330052303761005\n",
            "Epoch: 94, Loss: 0.09295907797676048\n",
            "Epoch: 95, Loss: 0.09262680538564617\n",
            "Epoch: 96, Loss: 0.09230345810443583\n",
            "Epoch: 97, Loss: 0.09198879564677757\n",
            "Epoch: 98, Loss: 0.0916825840193462\n",
            "Epoch: 99, Loss: 0.09138459554627225\n",
            "Epoch: 100, Loss: 0.09109460869833153\n",
            "Epoch: 101, Loss: 0.09081240792674732\n",
            "Epoch: 102, Loss: 0.09053778350150202\n",
            "Epoch: 103, Loss: 0.09027053135401637\n",
            "Epoch: 104, Loss: 0.09001045292409578\n",
            "Epoch: 105, Loss: 0.08975735501101109\n",
            "Epoch: 106, Loss: 0.08951104962861563\n",
            "Epoch: 107, Loss: 0.08927135386438066\n",
            "Epoch: 108, Loss: 0.08903808974224815\n",
            "Epoch: 109, Loss: 0.08881108408919539\n",
            "Epoch: 110, Loss: 0.0885901684054053\n",
            "Epoch: 111, Loss: 0.08837517873796079\n",
            "Epoch: 112, Loss: 0.08816595555794694\n",
            "Epoch: 113, Loss: 0.08796234364088505\n",
            "Epoch: 114, Loss: 0.08776419195039908\n",
            "Epoch: 115, Loss: 0.08757135352503288\n",
            "Epoch: 116, Loss: 0.0873836853681283\n",
            "Epoch: 117, Loss: 0.08720104834068214\n",
            "Epoch: 118, Loss: 0.08702330705710515\n",
            "Epoch: 119, Loss: 0.08685032978379738\n",
            "Epoch: 120, Loss: 0.08668198834047271\n",
            "Epoch: 121, Loss: 0.08651815800414736\n",
            "Epoch: 122, Loss: 0.08635871741573066\n",
            "Epoch: 123, Loss: 0.0862035484891389\n",
            "Epoch: 124, Loss: 0.08605253632287564\n",
            "Epoch: 125, Loss: 0.08590556911399257\n",
            "Epoch: 126, Loss: 0.08576253807438391\n",
            "Epoch: 127, Loss: 0.0856233373493482\n",
            "Epoch: 128, Loss: 0.08548786393834974\n",
            "Epoch: 129, Loss: 0.08535601761791703\n",
            "Epoch: 130, Loss: 0.08522770086664189\n",
            "Epoch: 131, Loss: 0.08510281879219049\n",
            "Epoch: 132, Loss: 0.08498127906030083\n",
            "Epoch: 133, Loss: 0.08486299182569391\n",
            "Epoch: 134, Loss: 0.08474786966485326\n",
            "Epoch: 135, Loss: 0.08463582751062848\n",
            "Epoch: 136, Loss: 0.08452678258860066\n",
            "Epoch: 137, Loss: 0.08442065435517171\n",
            "Epoch: 138, Loss: 0.08431736443732706\n",
            "Epoch: 139, Loss: 0.08421683657402756\n",
            "Epoch: 140, Loss: 0.08411899655918685\n",
            "Epoch: 141, Loss: 0.08402377218618663\n",
            "Epoch: 142, Loss: 0.08393109319389855\n",
            "Epoch: 143, Loss: 0.08384089121416191\n",
            "Epoch: 144, Loss: 0.08375309972067875\n",
            "Epoch: 145, Loss: 0.08366765397929572\n",
            "Epoch: 146, Loss: 0.08358449099962816\n",
            "Epoch: 147, Loss: 0.0835035494879889\n",
            "Epoch: 148, Loss: 0.08342476980159487\n",
            "Epoch: 149, Loss: 0.08334809390400916\n",
            "Epoch: 150, Loss: 0.08327346532178254\n",
            "Epoch: 151, Loss: 0.08320082910227809\n",
            "Epoch: 152, Loss: 0.08313013177262508\n",
            "Epoch: 153, Loss: 0.0830613212997869\n",
            "Epoch: 154, Loss: 0.08299434705170979\n",
            "Epoch: 155, Loss: 0.08292915975951946\n",
            "Epoch: 156, Loss: 0.08286571148073625\n",
            "Epoch: 157, Loss: 0.08280395556349224\n",
            "Epoch: 158, Loss: 0.08274384661170744\n",
            "Epoch: 159, Loss: 0.08268534045121245\n",
            "Epoch: 160, Loss: 0.08262839409678314\n",
            "Epoch: 161, Loss: 0.08257296572006814\n",
            "Epoch: 162, Loss: 0.0825190146183823\n",
            "Epoch: 163, Loss: 0.08246650118434186\n",
            "Epoch: 164, Loss: 0.0824153868763245\n",
            "Epoch: 165, Loss: 0.08236563418971952\n",
            "Epoch: 166, Loss: 0.08231720662896463\n",
            "Epoch: 167, Loss: 0.082270068680331\n",
            "Epoch: 168, Loss: 0.08222418578544297\n",
            "Epoch: 169, Loss: 0.08217952431552109\n",
            "Epoch: 170, Loss: 0.08213605154631064\n",
            "Epoch: 171, Loss: 0.08209373563369494\n",
            "Epoch: 172, Loss: 0.08205254558996371\n",
            "Epoch: 173, Loss: 0.0820124512607244\n",
            "Epoch: 174, Loss: 0.08197342330243475\n",
            "Epoch: 175, Loss: 0.08193543316054334\n",
            "Epoch: 176, Loss: 0.08189845304822314\n",
            "Epoch: 177, Loss: 0.08186245592567341\n",
            "Epoch: 178, Loss: 0.08182741547998565\n",
            "Epoch: 179, Loss: 0.08179330610555181\n",
            "Epoch: 180, Loss: 0.0817601028850004\n",
            "Epoch: 181, Loss: 0.08172778157064992\n",
            "Epoch: 182, Loss: 0.0816963185664579\n",
            "Epoch: 183, Loss: 0.08166569091046491\n",
            "Epoch: 184, Loss: 0.08163587625770494\n",
            "Epoch: 185, Loss: 0.08160685286358325\n",
            "Epoch: 186, Loss: 0.08157859956769788\n",
            "Epoch: 187, Loss: 0.08155109577810189\n",
            "Epoch: 188, Loss: 0.08152432145598891\n",
            "Epoch: 189, Loss: 0.08149825710079067\n",
            "Epoch: 190, Loss: 0.08147288373567875\n",
            "Epoch: 191, Loss: 0.08144818289345845\n",
            "Epoch: 192, Loss: 0.08142413660283998\n",
            "Epoch: 193, Loss: 0.0814007273750813\n",
            "Epoch: 194, Loss: 0.08137793819099566\n",
            "Epoch: 195, Loss: 0.08135575248830422\n",
            "Epoch: 196, Loss: 0.08133415414933072\n",
            "Epoch: 197, Loss: 0.08131312748903304\n",
            "Epoch: 198, Loss: 0.08129265724334993\n",
            "Epoch: 199, Loss: 0.08127272855787167\n",
            "Epoch: 200, Loss: 0.08125332697680628\n",
            "Epoch: 201, Loss: 0.0812344384322515\n",
            "Epoch: 202, Loss: 0.08121604923375034\n",
            "Epoch: 203, Loss: 0.08119814605813427\n",
            "Epoch: 204, Loss: 0.08118071593963513\n",
            "Epoch: 205, Loss: 0.08116374626026629\n",
            "Epoch: 206, Loss: 0.08114722474046206\n",
            "Epoch: 207, Loss: 0.08113113942997249\n",
            "Epoch: 208, Loss: 0.08111547869899946\n",
            "Epoch: 209, Loss: 0.08110023122957456\n",
            "Epoch: 210, Loss: 0.08108538600716979\n",
            "Epoch: 211, Loss: 0.08107093231253133\n",
            "Epoch: 212, Loss: 0.08105685971374062\n",
            "Epoch: 213, Loss: 0.08104315805847795\n",
            "Epoch: 214, Loss: 0.08102981746650663\n",
            "Epoch: 215, Loss: 0.08101682832235332\n",
            "Epoch: 216, Loss: 0.08100418126818539\n",
            "Epoch: 217, Loss: 0.0809918671968846\n",
            "Epoch: 218, Loss: 0.08097987724530142\n",
            "Epoch: 219, Loss: 0.08096820278769791\n",
            "Epoch: 220, Loss: 0.08095683542936266\n",
            "Epoch: 221, Loss: 0.08094576700039789\n",
            "Epoch: 222, Loss: 0.08093498954967783\n",
            "Epoch: 223, Loss: 0.08092449533896576\n",
            "Epoch: 224, Loss: 0.08091427683719049\n",
            "Epoch: 225, Loss: 0.08090432671488156\n",
            "Epoch: 226, Loss: 0.08089463783874677\n",
            "Epoch: 227, Loss: 0.08088520326640117\n",
            "Epoch: 228, Loss: 0.08087601624123707\n",
            "Epoch: 229, Loss: 0.08086707018743093\n",
            "Epoch: 230, Loss: 0.08085835870508877\n",
            "Epoch: 231, Loss: 0.08084987556551466\n",
            "Epoch: 232, Loss: 0.08084161470661347\n",
            "Epoch: 233, Loss: 0.08083357022841856\n",
            "Epoch: 234, Loss: 0.08082573638873139\n",
            "Epoch: 235, Loss: 0.08081810759888651\n",
            "Epoch: 236, Loss: 0.08081067841962966\n",
            "Epoch: 237, Loss: 0.08080344355710059\n",
            "Epoch: 238, Loss: 0.08079639785893525\n",
            "Epoch: 239, Loss: 0.08078953631045996\n",
            "Epoch: 240, Loss: 0.08078285403099965\n",
            "Epoch: 241, Loss: 0.0807763462702772\n",
            "Epoch: 242, Loss: 0.0807700084049139\n",
            "Epoch: 243, Loss: 0.08076383593502427\n",
            "Epoch: 244, Loss: 0.08075782448089994\n",
            "Epoch: 245, Loss: 0.08075196977978535\n",
            "Epoch: 246, Loss: 0.08074626768273886\n",
            "Epoch: 247, Loss: 0.0807407141515779\n",
            "Epoch: 248, Loss: 0.08073530525590948\n",
            "Epoch: 249, Loss: 0.08073003717023525\n",
            "Epoch: 250, Loss: 0.0807249061711416\n",
            "Epoch: 251, Loss: 0.080719908634556\n",
            "Epoch: 252, Loss: 0.08071504103308752\n",
            "Epoch: 253, Loss: 0.08071029993343153\n",
            "Epoch: 254, Loss: 0.08070568199384462\n",
            "Epoch: 255, Loss: 0.08070118396169605\n",
            "Epoch: 256, Loss: 0.08069680267106985\n",
            "Epoch: 257, Loss: 0.08069253504044725\n",
            "Epoch: 258, Loss: 0.08068837807044026\n",
            "Epoch: 259, Loss: 0.08068432884158985\n",
            "Epoch: 260, Loss: 0.08068038451222895\n",
            "Epoch: 261, Loss: 0.08067654231638915\n",
            "Epoch: 262, Loss: 0.08067279956178032\n",
            "Epoch: 263, Loss: 0.08066915362781281\n",
            "Epoch: 264, Loss: 0.08066560196367484\n",
            "Epoch: 265, Loss: 0.08066214208646719\n",
            "Epoch: 266, Loss: 0.0806587715793817\n",
            "Epoch: 267, Loss: 0.08065548808993107\n",
            "Epoch: 268, Loss: 0.08065228932822735\n",
            "Epoch: 269, Loss: 0.08064917306530416\n",
            "Epoch: 270, Loss: 0.0806461371314881\n",
            "Epoch: 271, Loss: 0.0806431794148089\n",
            "Epoch: 272, Loss: 0.0806402978594553\n",
            "Epoch: 273, Loss: 0.08063749046427335\n",
            "Epoch: 274, Loss: 0.08063475528130193\n",
            "Epoch: 275, Loss: 0.08063209041434961\n",
            "Epoch: 276, Loss: 0.08062949401760874\n",
            "Epoch: 277, Loss: 0.08062696429431175\n",
            "Epoch: 278, Loss: 0.08062449949541198\n",
            "Epoch: 279, Loss: 0.08062209791831373\n",
            "Epoch: 280, Loss: 0.08061975790562619\n",
            "Epoch: 281, Loss: 0.08061747784395625\n",
            "Epoch: 282, Loss: 0.08061525616273182\n",
            "Epoch: 283, Loss: 0.08061309133305601\n",
            "Epoch: 284, Loss: 0.0806109818665939\n",
            "Epoch: 285, Loss: 0.08060892631448849\n",
            "Epoch: 286, Loss: 0.08060692326630495\n",
            "Epoch: 287, Loss: 0.08060497134900542\n",
            "Epoch: 288, Loss: 0.0806030692259465\n",
            "Epoch: 289, Loss: 0.08060121559591114\n",
            "Epoch: 290, Loss: 0.08059940919215985\n",
            "Epoch: 291, Loss: 0.08059764878150803\n",
            "Epoch: 292, Loss: 0.08059593316343558\n",
            "Epoch: 293, Loss: 0.08059426116920887\n",
            "Epoch: 294, Loss: 0.08059263166103468\n",
            "Epoch: 295, Loss: 0.08059104353123465\n",
            "Epoch: 296, Loss: 0.08058949570143994\n",
            "Epoch: 297, Loss: 0.08058798712181052\n",
            "Epoch: 298, Loss: 0.08058651677027369\n",
            "Epoch: 299, Loss: 0.0805850836517814\n",
            "Epoch: 300, Loss: 0.08058368679759151\n",
            "Epoch: 301, Loss: 0.0805823252645669\n",
            "Epoch: 302, Loss: 0.08058099813448748\n",
            "Epoch: 303, Loss: 0.08057970451339314\n",
            "Epoch: 304, Loss: 0.08057844353093122\n",
            "Epoch: 305, Loss: 0.08057721433972936\n",
            "Epoch: 306, Loss: 0.0805760161147835\n",
            "Epoch: 307, Loss: 0.08057484805286026\n",
            "Epoch: 308, Loss: 0.08057370937191861\n",
            "Epoch: 309, Loss: 0.08057259931054372\n",
            "Epoch: 310, Loss: 0.08057151712739853\n",
            "Epoch: 311, Loss: 0.08057046210068912\n",
            "Epoch: 312, Loss: 0.08056943352764237\n",
            "Epoch: 313, Loss: 0.08056843072400385\n",
            "Epoch: 314, Loss: 0.08056745302353856\n",
            "Epoch: 315, Loss: 0.08056649977755832\n",
            "Epoch: 316, Loss: 0.08056557035445004\n",
            "Epoch: 317, Loss: 0.08056466413922142\n",
            "Epoch: 318, Loss: 0.08056378053306266\n",
            "Epoch: 319, Loss: 0.080562918952912\n",
            "Epoch: 320, Loss: 0.08056207883103887\n",
            "Epoch: 321, Loss: 0.08056125961463557\n",
            "Epoch: 322, Loss: 0.0805604607654219\n",
            "Epoch: 323, Loss: 0.08055968175925655\n",
            "Epoch: 324, Loss: 0.08055892208576475\n",
            "Epoch: 325, Loss: 0.08055818124796889\n",
            "Epoch: 326, Loss: 0.08055745876193462\n",
            "Epoch: 327, Loss: 0.08055675415642613\n",
            "Epoch: 328, Loss: 0.08055606697256423\n",
            "Epoch: 329, Loss: 0.08055539676350344\n",
            "Epoch: 330, Loss: 0.08055474309410897\n",
            "Epoch: 331, Loss: 0.0805541055406467\n",
            "Epoch: 332, Loss: 0.08055348369048236\n",
            "Epoch: 333, Loss: 0.08055287714178583\n",
            "Epoch: 334, Loss: 0.08055228550324418\n",
            "Epoch: 335, Loss: 0.0805517083937838\n",
            "Epoch: 336, Loss: 0.080551145442296\n",
            "Epoch: 337, Loss: 0.08055059628737919\n",
            "Epoch: 338, Loss: 0.08055006057707445\n",
            "Epoch: 339, Loss: 0.08054953796862052\n",
            "Epoch: 340, Loss: 0.08054902812820727\n",
            "Epoch: 341, Loss: 0.08054853073073992\n",
            "Epoch: 342, Loss: 0.08054804545960795\n",
            "Epoch: 343, Loss: 0.08054757200645957\n",
            "Epoch: 344, Loss: 0.08054711007098685\n",
            "Epoch: 345, Loss: 0.08054665936070661\n",
            "Epoch: 346, Loss: 0.0805462195907592\n",
            "Epoch: 347, Loss: 0.08054579048370414\n",
            "Epoch: 348, Loss: 0.08054537176932385\n",
            "Epoch: 349, Loss: 0.08054496318443403\n",
            "Epoch: 350, Loss: 0.08054456447269658\n",
            "Epoch: 351, Loss: 0.08054417538443881\n",
            "Epoch: 352, Loss: 0.08054379567647851\n",
            "Epoch: 353, Loss: 0.08054342511195112\n",
            "Epoch: 354, Loss: 0.08054306346014385\n",
            "Epoch: 355, Loss: 0.0805427104963324\n",
            "Epoch: 356, Loss: 0.08054236600162457\n",
            "Epoch: 357, Loss: 0.08054202976280432\n",
            "Epoch: 358, Loss: 0.08054170157218286\n",
            "Epoch: 359, Loss: 0.08054138122745418\n",
            "Epoch: 360, Loss: 0.08054106853155135\n",
            "Epoch: 361, Loss: 0.08054076329250938\n",
            "Epoch: 362, Loss: 0.08054046532332997\n",
            "Epoch: 363, Loss: 0.08054017444185246\n",
            "Epoch: 364, Loss: 0.08053989047062399\n",
            "Epoch: 365, Loss: 0.08053961323677761\n",
            "Epoch: 366, Loss: 0.08053934257191041\n",
            "Epoch: 367, Loss: 0.08053907831196655\n",
            "Epoch: 368, Loss: 0.08053882029712324\n",
            "Epoch: 369, Loss: 0.08053856837167778\n",
            "Epoch: 370, Loss: 0.08053832238394254\n",
            "Epoch: 371, Loss: 0.08053808218613583\n",
            "Epoch: 372, Loss: 0.08053784763428107\n",
            "Epoch: 373, Loss: 0.08053761858810708\n",
            "Epoch: 374, Loss: 0.08053739491094934\n",
            "Epoch: 375, Loss: 0.08053717646965752\n",
            "Epoch: 376, Loss: 0.08053696313450166\n",
            "Epoch: 377, Loss: 0.08053675477908298\n",
            "Epoch: 378, Loss: 0.08053655128024546\n",
            "Epoch: 379, Loss: 0.0805363525179932\n",
            "Epoch: 380, Loss: 0.08053615837540569\n",
            "Epoch: 381, Loss: 0.08053596873855667\n",
            "Epoch: 382, Loss: 0.08053578349643925\n",
            "Epoch: 383, Loss: 0.08053560254088392\n",
            "Epoch: 384, Loss: 0.08053542576649053\n",
            "Epoch: 385, Loss: 0.08053525307055057\n",
            "Epoch: 386, Loss: 0.08053508435297858\n",
            "Epoch: 387, Loss: 0.08053491951624668\n",
            "Epoch: 388, Loss: 0.08053475846531212\n",
            "Epoch: 389, Loss: 0.08053460110755663\n",
            "Epoch: 390, Loss: 0.08053444735272225\n",
            "Epoch: 391, Loss: 0.08053429711284747\n",
            "Epoch: 392, Loss: 0.0805341503022101\n",
            "Epoch: 393, Loss: 0.08053400683726823\n",
            "Epoch: 394, Loss: 0.08053386663660159\n",
            "Epoch: 395, Loss: 0.0805337296208586\n",
            "Epoch: 396, Loss: 0.08053359571270136\n",
            "Epoch: 397, Loss: 0.08053346483675358\n",
            "Epoch: 398, Loss: 0.08053333691954759\n",
            "Epoch: 399, Loss: 0.08053321188947898\n",
            "Epoch: 400, Loss: 0.08053308967675303\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialising W and b\n",
        "W = np.zeros((n_targets, n_features))\n",
        "b = np.zeros((n_targets,1))\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "# Initialising the learning rate and  epoch\n",
        "learning_rate =   0.02  #@param {type:\"number\"}\n",
        "num_epochs = 400  #@param {type:\"slider\", min:1, max:400, step:1}\n",
        "\n",
        "for epoch in range(num_epochs):  \n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        x_i, y_i = x_train[i].reshape(-1, 1), y_train[i].reshape(-1, 1)\n",
        "        \n",
        "        y_hat = (np.dot(W, x_i) + b)\n",
        "        \n",
        "        dL_dy = 2*(y_hat-y_i)\n",
        "        \n",
        "        dy_dW = x_i\n",
        "        dy_db = 1\n",
        "        \n",
        "        dL_dW = np.dot(dL_dy, dy_dW.T)\n",
        "        dL_db = dL_dy * dy_db\n",
        "        \n",
        "        #  Gradient Descend\n",
        "        W = W - learning_rate * dL_dW\n",
        "        b = b - learning_rate * dL_db\n",
        "        \n",
        "        loss += loss_fn(y_hat, y_i)\n",
        "    \n",
        "    loss_history.append(loss)\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss}\")\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RTmgAzIXgXca"
      },
      "outputs": [],
      "source": [
        "# Creating a prediction Model\n",
        "def Linear_Regression(x):\n",
        "  return (np.dot(W, x) + b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_81vDsQvjD5a",
        "outputId": "3d2881cf-9ad4-4222-fea3-c98beecf8854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9325287117072677\n"
          ]
        }
      ],
      "source": [
        "predict=[]\n",
        "\n",
        "# Predicting the values for test samples\n",
        "for i in range(n_total-n_samples):\n",
        "  predict.append(Linear_Regression(x_test[i].reshape(-1,1)))\n",
        "\n",
        "predict=np.array(predict)\n",
        "predict.shape=(1914,)\n",
        "\n",
        "# Finding the R squared value for accuracy\n",
        "corr_matrix = np.corrcoef(y_test, predict)\n",
        "corr = corr_matrix[0,1]\n",
        "R_sq = corr**2\n",
        " \n",
        "print(R_sq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnegCU2uJPFJ",
        "outputId": "0b0cdeb8-a9d3-41cd-f1a2-b33f594583d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of W : (4,)\n",
            "Shape of b : ()\n",
            "\n",
            "Loss = 0.07219315524722159\n"
          ]
        }
      ],
      "source": [
        "# Usig Sklearn's Linear Regression model and fitting out training data in it\n",
        "regression.fit(x_train, y_train)\n",
        "W_1, b_1 = regression.coef_, regression.intercept_\n",
        "\n",
        "print(\"Shape of W : {}\".format(W_1.shape))\n",
        "print(\"Shape of b : {}\".format(b_1.shape))\n",
        "\n",
        "loss = 0\n",
        "for i in range(n_samples):\n",
        "    y_hat = np.dot(W_1, x_train[i]) + b_1\n",
        "    loss += loss_fn(y_hat, y_train[i])\n",
        "\n",
        "print(\"\\nLoss = {}\".format(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B36cPTNAF_ZT",
        "outputId": "c7deea13-9562-46e0-c438-521bd96dd3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9325336829466628\n"
          ]
        }
      ],
      "source": [
        "predict_1=[]\n",
        "\n",
        "#Predicting the test values for the Sklearn Linear regression model\n",
        "for i in range(n_total-n_samples):\n",
        "  predict_1.append(np.dot(W_1,x_test[i].reshape(-1,1)+ b_1))\n",
        "\n",
        "predict_1=np.array(predict_1)\n",
        "predict_1.shape=(1914,)\n",
        "\n",
        "# Finding the R squared value for accuracy of the model\n",
        "corr_matrix = np.corrcoef(y_test, predict_1)\n",
        "corr = corr_matrix[0,1]\n",
        "R_sq = corr**2\n",
        " \n",
        "print(R_sq)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Linear_Reg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
